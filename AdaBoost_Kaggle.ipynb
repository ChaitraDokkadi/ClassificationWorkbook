{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.14410032e+08  6.15648284e+06 -4.00090829e+04 ...  7.00000000e-06\n",
      "  -1.20800000e-03  0.00000000e+00]\n",
      " [ 8.88722706e+08 -5.69765304e+06 -3.24635045e+05 ...  6.41300000e-03\n",
      "  -2.12300000e-03  0.00000000e+00]\n",
      " [-1.83404456e+09  2.01289646e+06 -5.75750764e+04 ...  4.26300000e-03\n",
      "  -9.48200000e-03  1.00000000e+00]\n",
      " ...\n",
      " [ 2.18199942e+09 -2.71792293e+06 -2.81535231e+05 ...  6.31000000e-04\n",
      "   6.54000000e-04  0.00000000e+00]\n",
      " [-1.03737127e+09 -9.74416084e+04 -4.18064774e+04 ...  3.52000000e-04\n",
      "  -3.07000000e-04  1.00000000e+00]\n",
      " [ 3.20748843e+09 -4.19769626e+06 -2.91553136e+05 ... -5.18000000e-04\n",
      "  -6.60000000e-05  0.00000000e+00]]\n",
      "418\n",
      "378\n",
      "0\n",
      "418\n",
      "418\n",
      "1\n",
      "418\n",
      "418\n",
      "2\n",
      "418\n",
      "418\n",
      "3\n",
      "418\n",
      "418\n",
      "4\n",
      "418\n",
      "418\n",
      "5\n",
      "418\n",
      "418\n",
      "6\n",
      "418\n",
      "418\n",
      "7\n",
      "418\n",
      "418\n",
      "8\n",
      "418\n",
      "418\n",
      "9\n",
      "418\n",
      "418\n",
      "10\n",
      "418\n",
      "418\n",
      "11\n",
      "418\n",
      "418\n",
      "0.8930942120892998\n",
      "0.7220158671863359\n",
      "0.7709403847104861\n",
      "0.6833522319772704\n",
      "0.5089948271108193\n",
      "0.7518773215954034\n",
      "0.7012328730518924\n",
      "0.9550262424358946\n",
      "0.8659025108546807\n",
      "0.6421358849484815\n",
      "0.7911687250965221\n",
      "0.7636210284843519\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1.]\n",
      "378\n",
      "378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:194: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def gini_index(samples, classes):\n",
    "    gini = 0.0\n",
    "    samples_count = float(sum([len(group) for group in samples]))\n",
    "    \n",
    "    for group in samples:\n",
    "        if float(len(group)) < 1.0:\n",
    "            continue\n",
    "        total_score = 0.0\n",
    "        for current_class in classes:\n",
    "            current_class_score = [row[-1] for row in group].count(current_class) / float(len(group))\n",
    "            total_score += current_class_score**2\n",
    "        gini += (1.0 - total_score) * (float(len(group)) / samples_count)\n",
    "    return gini\n",
    "\n",
    "def evaluate_best_split(data):\n",
    "    class_values = np.unique(data[:,-1])\n",
    "    \n",
    "    best_index = 10000\n",
    "    best_value = 10000\n",
    "    best_score = 10000\n",
    "    best_groups = None\n",
    "    \n",
    "    for index in range(len(data[0])-1):\n",
    "        for row in data:\n",
    "            left = []\n",
    "            right = []\n",
    "            for item in data:\n",
    "                if type(row[index]) is str:\n",
    "                    left.append(item) if item[index] == row[index] else right.append(item)\n",
    "                else:\n",
    "                    left.append(item) if item[index] <= row[index] else right.append(item)\n",
    "            ordered_groups = np.array(left), np.array(right)\n",
    "            \n",
    "            gini = gini_index(ordered_groups, class_values)\n",
    "            if gini < best_score:\n",
    "                best_index = index\n",
    "                best_value = row[index]\n",
    "                best_score = gini\n",
    "                best_groups = ordered_groups\n",
    "    node = {}\n",
    "    node['index'] = best_index\n",
    "    node['value'] = best_value\n",
    "    node['left'], node['right'] = best_groups\n",
    "    return node\n",
    "\n",
    "def splitting(node,depth):\n",
    "    left = node['left']\n",
    "    right = node['right']\n",
    "    if left.size==0 or right.size==0:\n",
    "        classes = left[:,-1] if right.size == 0 else right[:,-1]\n",
    "    \n",
    "        node['left'] = max(classes, key = list(classes).count)\n",
    "        node['right'] = node['left']\n",
    "        return\n",
    "\n",
    "    if depth >= maximum_depth:\n",
    "        classes = left[:,-1]\n",
    "        node['left'] = max(classes, key = list(classes).count)\n",
    "        classes = right[:,-1]\n",
    "        node['right'] = max(classes, key = list(classes).count)\n",
    "        return\n",
    "    \n",
    "    if len(left) > 1:\n",
    "        node['left'] = evaluate_best_split(left)\n",
    "        splitting(node['left'],depth+1)\n",
    "    else:\n",
    "        classes = left[:,-1]\n",
    "        node['left'] = max(classes, key = list(classes).count)\n",
    "    \n",
    "    if len(right) > 1:\n",
    "        node['right'] = evaluate_best_split(right)\n",
    "        splitting(node['right'],depth+1)\n",
    "    else:    \n",
    "        classes = right[:,-1]\n",
    "        node['right'] = max(classes, key = list(classes).count)\n",
    "\n",
    "def get_decision_tree(train_data,depth):\n",
    "    root = evaluate_best_split(train_data)\n",
    "    splitting(root,depth)\n",
    "    return root\n",
    "\n",
    "def get_label(row,node):\n",
    "    if row[node['index']] >= node['value']:\n",
    "        return get_label(row,node['right']) if type(node['right']) is dict else node['right']\n",
    "    else:\n",
    "        return get_label(row,node['left']) if type(node['left']) is dict else node['left']\n",
    "        \n",
    "def get_labels(test_data,node):\n",
    "    labels = []\n",
    "    for row in test_data:\n",
    "        label = get_label(row,node)\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def get_labels_ada_boost(test_data,trees,alphas):\n",
    "    x = np.zeros(len(test_data))\n",
    "    for (m, a) in zip(trees, alphas):\n",
    "        print(a)\n",
    "        labels = get_labels(test_data,m)\n",
    "        updated_labels = []\n",
    "        for label in labels:\n",
    "            y = -1 if label == 0 else 1\n",
    "            updated_labels.append(y)\n",
    "        x += a * np.array(updated_labels) \n",
    "    return np.sign(x)\n",
    "\n",
    "def ada_Boost(train_data,m):\n",
    "    trees = []\n",
    "    alphas = []\n",
    "    \n",
    "    weights = np.ones(len(train_data)) / len(train_data)\n",
    "    \n",
    "    while len(trees) < m:\n",
    "#         print(len(trees))\n",
    "        N = len(weights)\n",
    "        \n",
    "        new_train_data_indices = np.random.choice(N, size=N, replace=True, p=weights)\n",
    "        \n",
    "        new_train_data = train_data[new_train_data_indices]\n",
    "\n",
    "        tree = get_decision_tree(new_train_data,0)\n",
    "        predicted_labels = get_labels(train_data,tree)\n",
    "        \n",
    "        error = weights.dot(predicted_labels != train_data[:,-1])\n",
    "        \n",
    "        if error > 0.5:\n",
    "            continue\n",
    "            \n",
    "        alpha = 0.5 * (np.log((1 - error)/error))\n",
    "#         print(len(predicted_labels))\n",
    "#         print(len(train_data[:,-1]))\n",
    "        train_labels = train_data[:,-1]\n",
    "        \n",
    "        change = []\n",
    "        for index in range(len(train_labels)):\n",
    "            change.append(1 if train_labels[index] == predicted_labels[index] else -1)\n",
    "        change = np.array(change)  \n",
    "\n",
    "        change = np.asarray(change)\n",
    "        weights *= np.exp(-alpha*change)\n",
    "        weights /= sum(weights)\n",
    "\n",
    "        trees.append(tree)\n",
    "        alphas.append(alpha)\n",
    "    \n",
    "    return trees,alphas\n",
    "\n",
    "train_label=\"train_label.csv\"\n",
    "train_label= np.loadtxt(train_label, delimiter=\",\",dtype='str')\n",
    "train_label=np.delete(train_label,0,1)\n",
    "train_label=np.delete(train_label,0,0)\n",
    "train_label=train_label.astype(np.float)\n",
    "\n",
    "file_name=\"train_features.csv\"\n",
    "data = np.loadtxt(file_name, delimiter=\",\",dtype='str')\n",
    "data =np.asarray(data)\n",
    "train_data=data.astype(np.float)\n",
    "train_data=np.delete(train_data,0,1)\n",
    "train_data = np.append(train_data,train_label,axis=1)\n",
    "\n",
    "test_features=\"test_features.csv\"\n",
    "test_data= np.loadtxt(test_features, delimiter=\",\",dtype='str')\n",
    "test_data=test_data.astype(np.float)\n",
    "x = np.reshape(test_data[:,0],(len(test_data),1))\n",
    "test_data=np.delete(test_data,0,1)\n",
    "\n",
    "M = [12]\n",
    "K=1\n",
    "maximum_depth = 4\n",
    "\n",
    "for m in M:\n",
    "    for i in range(K):\n",
    "#         print(len(train_data))\n",
    "#         print(len(test_data))\n",
    "        \n",
    "        trees,alphas = ada_Boost(train_data,m)\n",
    "        \n",
    "        predicted_labels = get_labels_ada_boost(test_data,trees,alphas)\n",
    "        \n",
    "        predicted_labels[predicted_labels == -1] = 0\n",
    "        predicted_labels[predicted_labels == 0] = '0'\n",
    "        predicted_labels[predicted_labels == 1] == '1'\n",
    "        print(predicted_labels)\n",
    "\n",
    "y = np.reshape(predicted_labels, len(predicted_labels),1)\n",
    "# print(predicted_labels)\n",
    "\n",
    "y = np.reshape(predicted_labels, (len(predicted_labels),1))\n",
    "# print(len(x))\n",
    "# print(len(y))\n",
    "z = np.append(x,y,axis=1)\n",
    "z = pd.DataFrame({'id': z[:, 0], 'label': z[:, 1]})\n",
    "# z.astype(int)\n",
    "z.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
